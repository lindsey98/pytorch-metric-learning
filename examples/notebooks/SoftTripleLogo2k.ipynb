{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul  1 10:23:17 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.80       Driver Version: 460.80       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN V             Off  | 00000000:3B:00.0 Off |                  N/A |\n",
      "| 28%   41C    P2    97W / 250W |   8730MiB / 12066MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN V             Off  | 00000000:5E:00.0 Off |                  N/A |\n",
      "| 28%   43C    P2    91W / 250W |   5428MiB / 12066MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    757576      C   .../envs/mypy37-2/bin/python     2411MiB |\n",
      "|    0   N/A  N/A   3992545      C   ...a3/envs/mypy37/bin/python     6315MiB |\n",
      "|    1   N/A  N/A    757576      C   .../envs/mypy37-2/bin/python     2453MiB |\n",
      "|    1   N/A  N/A   3992545      C   ...a3/envs/mypy37/bin/python     2971MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/l/liny/ruofan/pytorch-metric-learning/src')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0, 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_metric_learning import losses, miners, distances, reducers, testers\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "from torchvision import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "from pytorch_metric_learning.models import bninception\n",
    "from pytorch_metric_learning import samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "result_dir = './log'\n",
    "exp_name = 'logo2k'\n",
    "os.makedirs(result_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from PIL import Image, ImageOps\n",
    "from typing import List, Union, Callable\n",
    "\n",
    "class GetLoader(torch.utils.data.Dataset):\n",
    "    '''Define customized dataset\n",
    "    Args:\n",
    "        data_root:\n",
    "            Path to directory holding the images to load.\n",
    "        data_list:\n",
    "            Path to txt file which map images to labels.\n",
    "        label_dict:\n",
    "            Dict which converts label in plain text to label in int.\n",
    "        transform:\n",
    "            Transformations to apply.\n",
    "        grayscale:\n",
    "            Grayscale model/RGB model, default is RGB\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, data_root, data_list, label_dict, transform=None, grayscale=False):\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.data_root = data_root\n",
    "        self.grayscale = grayscale\n",
    "        data_list = [x.strip('\\n') for x in open(data_list).readlines()]\n",
    "\n",
    "        with open(label_dict, 'rb') as handle:\n",
    "            self.label_dict = pickle.load(handle)\n",
    "\n",
    "        self.classes = list(self.label_dict.keys())\n",
    "\n",
    "        self.n_data = len(data_list)\n",
    "\n",
    "        self.img_paths = []\n",
    "        self.labels = []\n",
    "        self.targets = []\n",
    "\n",
    "        for data in data_list:\n",
    "            image_path = data\n",
    "            label = image_path.split('/')[0]\n",
    "            self.img_paths.append(image_path)\n",
    "            self.labels.append(label)\n",
    "            self.targets.append(self.label_dict[label])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img_path, label= self.img_paths[index], self.labels[index]\n",
    "        img_path_full = os.path.join(self.data_root, img_path)\n",
    "        if self.grayscale:\n",
    "            img = Image.open(img_path_full).convert('L').convert('RGB')\n",
    "        else:\n",
    "            img = Image.open(img_path_full).convert('RGB')\n",
    "\n",
    "        img = ImageOps.expand(img, (\n",
    "            (max(img.size) - img.size[0]) // 2, (max(img.size) - img.size[1]) // 2,\n",
    "            (max(img.size) - img.size[0]) // 2, (max(img.size) - img.size[1]) // 2), fill=(255, 255, 255))\n",
    "\n",
    "        label = self.label_dict[label]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = GetLoader(data_root='/home/l/liny/ruofan/lightly/datasets/logo2k/train/', \n",
    "                           data_list='/home/l/liny/ruofan/PhishIntention/src/siamese_retrain/logo2k/train.txt', \n",
    "                           label_dict='/home/l/liny/ruofan/PhishIntention/src/siamese_retrain/logo2k/logo2k_labeldict.pkl',\n",
    "                           transform=train_transform)\n",
    "\n",
    "test_data = GetLoader(data_root='/home/l/liny/ruofan/lightly/datasets/logo2k/test/', \n",
    "                      data_list='/home/l/liny/ruofan/PhishIntention/src/siamese_retrain/logo2k/test.txt', \n",
    "                      label_dict='/home/l/liny/ruofan/PhishIntention/src/siamese_retrain/logo2k/logo2k_labeldict.pkl',\n",
    "                      transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = samplers.MPerClassSampler(train_data.labels, \n",
    "                                    m=5, \n",
    "                                    length_before_new_iter=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=sampler)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bninception(dim=512, pretrained=None)\n",
    "model = torch.nn.DataParallel(model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ### \n",
    "def train(model, loss_func, mining_func, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(data)\n",
    "        indices_tuple = mining_func(embeddings, labels)\n",
    "        loss = loss_func(embeddings, labels, indices_tuple)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\"Epoch {} Iteration {}: Loss = {}, Number of mined triplets = {}\".format(epoch, batch_idx, loss, mining_func.num_triplets))\n",
    "\n",
    "### convenient function from pytorch-metric-learning ###\n",
    "def get_all_embeddings(dataset, model):\n",
    "    tester = testers.BaseTester()\n",
    "    return tester.get_all_embeddings(dataset, model)\n",
    "\n",
    "### compute accuracy using AccuracyCalculator from pytorch-metric-learning ###\n",
    "def test(train_set, test_set, model, accuracy_calculator):\n",
    "    train_embeddings, train_labels = get_all_embeddings(train_set, model)\n",
    "    test_embeddings, test_labels = get_all_embeddings(test_set, model)\n",
    "    print(\"Computing accuracy\")\n",
    "    accuracies = accuracy_calculator.get_accuracy(test_embeddings, \n",
    "                                                  train_embeddings,\n",
    "                                                  test_labels,\n",
    "                                                  train_labels,\n",
    "                                                  False)\n",
    "    print(\"Test set accuracy (Precision@1) = {}\".format(accuracies[\"precision_at_1\"]))\n",
    "    return accuracies[\"precision_at_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pytorch-metric-learning stuff ###\n",
    "distance = distances.CosineSimilarity()\n",
    "reducer = reducers.MeanReducer()\n",
    "loss_func = losses.SoftTripleLoss(num_classes=2340, \n",
    "                                  embedding_size=512, \n",
    "                                  centers_per_class=5, \n",
    "                                  la=20, \n",
    "                                  gamma=0.1, \n",
    "                                  margin=0.01).to(device)\n",
    "\n",
    "loss_optimizer = torch.optim.Adam([{\"params\": model.parameters(), \"lr\": 1e-4},\n",
    "                                   {\"params\": loss_func.parameters(), \"lr\": 1e-2}])\n",
    "\n",
    "mining_func = miners.TripletMarginMiner(margin = 0.2, distance = distance, type_of_triplets = \"semi-hard\")\n",
    "accuracy_calculator = AccuracyCalculator(include = (\"precision_at_1\",), k = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iteration 0: Loss = 7.890983581542969, Number of mined triplets = 14702\n",
      "Epoch 1 Iteration 100: Loss = 6.437102794647217, Number of mined triplets = 10033\n",
      "Epoch 1 Iteration 200: Loss = 6.644566535949707, Number of mined triplets = 9564\n",
      "Epoch 1 Iteration 300: Loss = 5.138212203979492, Number of mined triplets = 7776\n",
      "Epoch 1 Iteration 400: Loss = 4.899921894073486, Number of mined triplets = 7424\n",
      "Epoch 1 Iteration 500: Loss = 6.552186012268066, Number of mined triplets = 10032\n",
      "Epoch 1 Iteration 600: Loss = 5.8222832679748535, Number of mined triplets = 9376\n",
      "Epoch 1 Iteration 700: Loss = 5.751879692077637, Number of mined triplets = 8978\n",
      "Epoch 1 Iteration 800: Loss = 5.512117385864258, Number of mined triplets = 9762\n",
      "Epoch 1 Iteration 900: Loss = 5.344035625457764, Number of mined triplets = 8410\n",
      "Epoch 1 Iteration 1000: Loss = 5.829801559448242, Number of mined triplets = 9519\n",
      "Epoch 1 Iteration 1100: Loss = 5.217018127441406, Number of mined triplets = 9072\n",
      "Epoch 1 Iteration 1200: Loss = 5.0307159423828125, Number of mined triplets = 8673\n",
      "Epoch 1 Iteration 1300: Loss = 5.88931941986084, Number of mined triplets = 9218\n",
      "Epoch 1 Iteration 1400: Loss = 4.602297306060791, Number of mined triplets = 7384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3655/3655 [04:13<00:00, 14.41it/s]\n",
      "100%|██████████| 1569/1569 [01:51<00:00, 14.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing accuracy\n",
      "Test set accuracy (Precision@1) = 0.06954684815354994\n",
      "Epoch 2 Iteration 0: Loss = 4.698849201202393, Number of mined triplets = 8346\n",
      "Epoch 2 Iteration 100: Loss = 5.577515602111816, Number of mined triplets = 8875\n",
      "Epoch 2 Iteration 200: Loss = 5.384181022644043, Number of mined triplets = 9619\n",
      "Epoch 2 Iteration 300: Loss = 5.572055339813232, Number of mined triplets = 9091\n",
      "Epoch 2 Iteration 400: Loss = 5.139688491821289, Number of mined triplets = 8790\n",
      "Epoch 2 Iteration 500: Loss = 5.322958946228027, Number of mined triplets = 9211\n",
      "Epoch 2 Iteration 600: Loss = 5.1449055671691895, Number of mined triplets = 8986\n",
      "Epoch 2 Iteration 700: Loss = 5.100333213806152, Number of mined triplets = 8374\n",
      "Epoch 2 Iteration 800: Loss = 4.989513397216797, Number of mined triplets = 8344\n",
      "Epoch 2 Iteration 900: Loss = 4.7517805099487305, Number of mined triplets = 8099\n",
      "Epoch 2 Iteration 1000: Loss = 5.105037689208984, Number of mined triplets = 8115\n",
      "Epoch 2 Iteration 1100: Loss = 4.63856315612793, Number of mined triplets = 8020\n",
      "Epoch 2 Iteration 1200: Loss = 3.976372718811035, Number of mined triplets = 6835\n",
      "Epoch 2 Iteration 1300: Loss = 4.756030082702637, Number of mined triplets = 8272\n",
      "Epoch 2 Iteration 1400: Loss = 4.619403839111328, Number of mined triplets = 8060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3655/3655 [04:20<00:00, 14.01it/s]\n",
      "100%|██████████| 1569/1569 [01:51<00:00, 14.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing accuracy\n",
      "Test set accuracy (Precision@1) = 0.0971663127784268\n",
      "Epoch 3 Iteration 0: Loss = 5.184262275695801, Number of mined triplets = 8535\n",
      "Epoch 3 Iteration 100: Loss = 4.109067916870117, Number of mined triplets = 6978\n",
      "Epoch 3 Iteration 200: Loss = 4.816425323486328, Number of mined triplets = 7332\n",
      "Epoch 3 Iteration 300: Loss = 5.017761707305908, Number of mined triplets = 7988\n",
      "Epoch 3 Iteration 400: Loss = 4.506860733032227, Number of mined triplets = 7442\n",
      "Epoch 3 Iteration 500: Loss = 4.686699867248535, Number of mined triplets = 7735\n",
      "Epoch 3 Iteration 600: Loss = 3.8762736320495605, Number of mined triplets = 6695\n",
      "Epoch 3 Iteration 700: Loss = 4.485928535461426, Number of mined triplets = 7946\n",
      "Epoch 3 Iteration 800: Loss = 5.441232681274414, Number of mined triplets = 8958\n",
      "Epoch 3 Iteration 900: Loss = 4.671576023101807, Number of mined triplets = 8057\n",
      "Epoch 3 Iteration 1000: Loss = 4.1937456130981445, Number of mined triplets = 7352\n",
      "Epoch 3 Iteration 1100: Loss = 5.602010250091553, Number of mined triplets = 8475\n",
      "Epoch 3 Iteration 1200: Loss = 5.556014060974121, Number of mined triplets = 8411\n",
      "Epoch 3 Iteration 1300: Loss = 4.689279556274414, Number of mined triplets = 7323\n",
      "Epoch 3 Iteration 1400: Loss = 5.512430191040039, Number of mined triplets = 9001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3655/3655 [04:20<00:00, 14.05it/s]\n",
      "100%|██████████| 1569/1569 [01:42<00:00, 15.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing accuracy\n",
      "Test set accuracy (Precision@1) = 0.12084013959974982\n",
      "Epoch 4 Iteration 0: Loss = 4.6623921394348145, Number of mined triplets = 7893\n",
      "Epoch 4 Iteration 100: Loss = 4.755434036254883, Number of mined triplets = 7540\n",
      "Epoch 4 Iteration 200: Loss = 4.282244682312012, Number of mined triplets = 7147\n",
      "Epoch 4 Iteration 300: Loss = 3.827988624572754, Number of mined triplets = 6538\n",
      "Epoch 4 Iteration 400: Loss = 3.767906665802002, Number of mined triplets = 6733\n",
      "Epoch 4 Iteration 500: Loss = 4.208225250244141, Number of mined triplets = 7634\n",
      "Epoch 4 Iteration 600: Loss = 5.026324272155762, Number of mined triplets = 8204\n",
      "Epoch 4 Iteration 700: Loss = 4.075446605682373, Number of mined triplets = 6745\n",
      "Epoch 4 Iteration 800: Loss = 4.452949523925781, Number of mined triplets = 7704\n",
      "Epoch 4 Iteration 900: Loss = 4.6066670417785645, Number of mined triplets = 7385\n",
      "Epoch 4 Iteration 1000: Loss = 4.591436862945557, Number of mined triplets = 7647\n",
      "Epoch 4 Iteration 1100: Loss = 4.2895426750183105, Number of mined triplets = 7701\n",
      "Epoch 4 Iteration 1200: Loss = 4.298622131347656, Number of mined triplets = 7710\n",
      "Epoch 4 Iteration 1300: Loss = 4.439873218536377, Number of mined triplets = 7079\n",
      "Epoch 4 Iteration 1400: Loss = 4.297041893005371, Number of mined triplets = 7896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3655/3655 [04:19<00:00, 14.08it/s]\n",
      "100%|██████████| 1569/1569 [01:53<00:00, 13.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing accuracy\n",
      "Test set accuracy (Precision@1) = 0.1493164851617621\n",
      "Epoch 5 Iteration 0: Loss = 3.8996593952178955, Number of mined triplets = 7348\n",
      "Epoch 5 Iteration 100: Loss = 4.021068096160889, Number of mined triplets = 7262\n",
      "Epoch 5 Iteration 200: Loss = 4.634955883026123, Number of mined triplets = 7663\n",
      "Epoch 5 Iteration 300: Loss = 3.544334650039673, Number of mined triplets = 5805\n"
     ]
    }
   ],
   "source": [
    "### pytorch-metric-learning stuff ###\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train(model, loss_func, mining_func, device, train_loader, loss_optimizer, epoch)\n",
    "    knn_acc = test(train_data, test_data, model, accuracy_calculator)\n",
    "    if epoch % 50 == 0 or epoch == 1:\n",
    "        torch.save(model.state_dict(), os.path.join(result_dir, '{}_epoch{}_knnAcc{:.4f}.pt'.format(exp_name, epoch, knn_acc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
